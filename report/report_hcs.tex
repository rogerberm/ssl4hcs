\RequirePackage[final]{graphicx} % to allow for graphics to show in draft mode
\documentclass[oneside, a4paper, draft]{memoir} % scrreprt | memoir [twocolumn?]
\usepackage[T1]{fontenc}  % encoding, ligature removal works
\usepackage{lipsum}
\usepackage[utf8]{inputenc}
\usepackage[obeyFinal]{todonotes}
\usepackage{ifdraft}
\usepackage{amsmath}
\usepackage{microtype}
\usepackage{listings}
\usepackage{blkarray}
\usepackage{mathpple}
\usepackage[comma, square, super]{natbib}
\usepackage{tabularx}
\usepackage{pdflscape}

\DisableLigatures{encoding = *, family = *}
\newcommand{\TODO}[1]{\todo[size=\tiny]{#1}}
\newcommand\imglabel[1]{\textbf{\textsf{#1}}}
\renewcommand{\rmdefault}{ptm}
\DeclareMathAlphabet\mathbfcal{OMS}{cmsy}{b}{n}


\setlrmarginsandblock{2.5cm}{2.5cm}{*}
\setulmarginsandblock{3cm}{3cm}{*}
\checkandfixthelayout

\title{
	Semi-supervised learning for phenotypic profiling of \mbox{high-content screens\ifdraft{ (DRAFT)}{\thanks{This
		project was held as a Lab Rotation in Computer Science, as required by the Master program in Computational
		Biology and Bioinformatics - ETH Z\"urich}}}}
\author{
	\ifdraft{Roger Bermudez-Chacon\\Supervisor: Peter Horvath}
	{Róger Bermúdez-Chacón\\\small Computational Biology and Bioinformatics Master program\\\\
			Supervisor: Peter Horvath\\\small Light Microscopy and Screening Centre}\\\\ETH Zurich
}
\date{\today}

\begin{document}
\maketitle
\begin{abstract}
Semi-supervised machine learning techniques are particularly useful in experiments where data annotation and
classification is time- and resource-consuming or error-prone. In biological experiments this is often the case.
Here, we apply a graph-based machine learning method to classify cells in different stages of infection with the
Semliki Forest Virus (SFV), which features have been extracted from image analysis of fluorescence microscopy results,
obtained in turn from a genome-wide high-content screening experiment.
The aim of this project is to investigate whether and to which extent intelligent control experiment design 
combined with semi-supervised learning can reach the accuracy of a human annotator and/or in certain 
cases substitute it.

\end{abstract}
\setcounter{secnumdepth}{0}

\section{Introduction}
Recent advancements in high-throughput microscopy and data analysis made possible to perform large 
scale biological experiments and automatically evaluate them. For the detection of sub-cellular changes 
caused by different perturbations in the cell (RNAi or drugs), often supervised machine learning (SML) 
is used. Reliable training of an SML method, however, requires significant effort from a field expert.

As an alternative, semi-supervised machine learning (SSL) methods make use of information intrinsically found in the 
entire data, both annotated and unannotated, thus allowing to make use of a larger amount of information
by exploiting, alongside with the annotated data, the relative distribution of unannotated data on the feature
space~\cite{chapelle2006semi}. Furthermore, common sources of error in manually-annotated data (subjective data 
interpretation by the annotator, instrument capabilities and calibration, noise, ...) can affect the
quality of the annotations, and in some cases SSL techniques can account for and correct such
misannotations\cite{du2010error}.

The semi-supervised learning paradigm, under a few assumptions\footnote{Smoothness, Cluster, and Manifold assumptions,
see~\cite[p.~4-6]{chapelle2006semi}}, has proven valuable in exploring and classifying biological data in fields as
diverse as drug-protein interactions~\cite{zheng2010semi}, gene expression~\cite{costa2007semi}, and medical
diagnosis~\cite{bair2004semi}.

\section{Materials and Methods}
\subsection{High-content screening}
A human genome-wide siRNA library was used to produce human cell cultures with knocked-out
genes, stored in a collection of 55 384-well plates spanning the entire genome. 
These cell cultures were exposed to a genetically engineered fluorescent SFV strand as a transduction vehicle for 
green fluorescent protein (GFP) genes, and the corresponding GFP production on all of the cultures was tracked over
time. 3 repetitions of this experiment were carried out.

The protein expression was stopped at \todo{t=?}$t$ after culture infection with SFV, for all siRNA-mediated
phenotypes, and microscopic pictures of the sample were obtained under a light microscope.
Wild-type cultures were also exposed to SFV, and protein expression was stopped at 4, 5, 6, and 7 hours after exposure
and analyzed as negative control experiments, in the exact same manner as for the silenced
samples. A control sample set for non-exposure to the virus was also collected and analyzed in the same way
as described above.


\subsection{Image acquisition and analysis}
For every sample at each infection stage, 9 tiled images were captured via a light microscope, by composing the
green fluorescent signal of the produced protein, and a blue-colored image of the nuclei.

Image segmentation of all microscopic pictures to identify individual cells, and the subsequent feature extraction per
identified cell, was done with CellProfiler~\cite{carpenter2006cellprofiler}.
A total of 93 features were retrieved and used in this experiment, corresponding to color intensity, area, shape, and
texture descriptors. (For the complete list of features extracted, see Appendix \ref{app:featurelist})

\subsection{Unannotated data}
The above process was performed automatically on the 380.160 images (55~plates $\times$ 384~wells $\times$ 
9~sites $\times$ 2~channels) retrieved from the siRNA-mediated phenotypes.
All 93 features\footnote{In total, 95 features were extracted
from the images. Spatial coordinates, however, were regarded as of little, if any value for data analysis in this
work.} were extracted as floating point values, and stored in text files, one line per cell. Unlabeled information for
around 100.000.000 cells was collected by this process.

\subsection{Annotated data}
From the genome-wide information, a small subset of the data was manually annotated by an expert on SFV infection, by
visually identifying cell phenotypes directly from the segmented microscopic images and cross-checking with the time
annotation on the respective source plate, and classifying them into the different stages of infection. This manual
point-and-click process yielded \todo{software reference for this?} 3098 annotated cells.\footnote{A small number of
imaging artifacts were also identified manually. However, accounting for such information was out of the scope of
this work.}

\subsection{Control data}
As control cultures, 5 plates treated with a control siRNA that has no effect on the expressed phenotype of the cells
were used. Control plates not exposed to SFV, as well as infected plates analyzed at 4, 5, 6, and 7 hours after
infection, were used to retrieve control information during the time course. The exact same image analysis and
feature extraction procedure described for the unannotated cultures was performed on these plates. 

\subsection{Semi-supervised learning implementation}
A graph-based label propagation (label spreading~\cite{zhou2004learning}) approach was followed. In this kind of
approach, an undirected graph is built using the data points (cells) as vertices, and edges are created for all pairs
of vertices that satisify a neighboring condition, with weights proportional to some measurement of association
between the pair of vertices. This degree of association is often assumed as related to the distance
between the data points in the n-dimensional feature space, in a linear, exponential, or Gaussian fashion, among
others, and can be either limited in space (k-nearest neighbors, cutoff distance, ...), or consist of a
complete graph that considers all possible pairwise relationships.

In the original formulation, a vector of initial labels $\hat{Y}^{(0)}$ is created by assigning the actual labels 
to the vertices corresponding to annotated data, and neutral labels to the unannotated data; then, in an iterative
fashion, described in equation~\ref{eq:zhou}, all vertices \emph{learn} the labels from their neighbors at a
rate $\alpha$ (and preserve their initial labeling at a rate $1-\alpha$). This process is repeated until convergence
to a $\hat{Y}^{(\infty)}$.

\begin{equation}
	\label{eq:zhou}
	\hat{Y}^{(t+1)} \leftarrow \alpha \mathbfcal{L}\hat{Y}^{(t)} + (1-\alpha)\hat{Y}^{(0)}
\end{equation}

% The above formulation resembles an isotropic model of diffusion.
The \emph{laplacian} term controlling the particular
strength of label spreading between any two data points is given by
$\mathbfcal{L} \leftarrow \mathbf{D}^{-1/2}\mathbf{W}\mathbf{D}^{-1/2}$, with $\mathbf{W}$ the
weight or affinity (distance-related) matrix between nodes in the graph (and $\mathbf{W}_{ii} = 0$), and
$\mathbf{D}$ the diagonal degree matrix $\mathbf{D}_{ii}~=~\sum_{j}{\mathbf{W}_{ij}}$. 

In the present implementation, the availability of \emph{experimentally supervised data} from the experimental control
samples was exploited. For each of these samples, the experimental conditions were known (in particular, the time after
exposure to the virus), and a specific phenotype (label) could therefore be expected. To make use of this information,
initial labels were assigned not only to the manually-annotated data points, but also to these experimentally
supervised data points, by using the labels corresponding to their expected infection phase.

The $n$ data points are classified in four types: labeled data,
experimentally-supervised data with high labeling confidence (data points taken from the uninfected control samples), 
experimentally-supervised data with standard labeling confidence (data points taken from the infected control
samples), and unlabeled data, with cardinalities $n_l$, $n_h$, $n_s$, $n_u$ respectively. Different learning rates
per each type ($\alpha_l$, $\alpha_h$, $\alpha_s$, and $\alpha_u$, respectively) were used to construct a diagonal
$n \times n$ matrix $\mathbfcal{A}$, such that $\mathbfcal{A}_{ii} \in \left\{ \alpha_l, \alpha_h, \alpha_s,
\alpha_u \right\}$, according to the class of the data point $i$. The actual values are found by hyperparameter
optimization (detailed in next sections).

The variant of the label spreading algorithm presented here, uses then the slightly modified (\emph{experimentally
supervised data}-aware) iterative formula:
\begin{equation}
	\hat{Y}^{(t+1)} \leftarrow \mathbfcal{LA}\hat{Y}^{(t)} + (\mathbf{I}-\mathbfcal{A})\hat{Y}^{(0)}
\end{equation}

The label spreading algorithm:
1. Read the $n \times f$ feature matrix $\mathbf{M}$ containing the $f$ selected feature values for all
$n = n_l + n_h + n_s + n_u$ data points.
2. Compute the affinity matrix $\textbf{W}$ from the pairwise distances between all data points in $M$
3. Create the initial label vector $\hat{Y}^{(0)}$ from all $n$ points, according to their type.
4. 
%, but in general, the configuration $\alpha_l \leq \alpha_h < \alpha_s < \alpha_u$ reflects the prior confidence in the data

%In the present implementation, prior knowledge of the nature of the data was incorporated as an additional level of
%\emph{soft labeling}, to exploit the fact that, for data points taken from the experimental control, the cells
%(vertices) can be tracked back to their experimental conditions and time course, which have a direct influence on what
%specific phenotypes (labels) are more likely to occur.

\subsubsection{Development and runtime environment}
To read and analyze the data, a script was coded in Python 2.7.4. Extensive use of the open source libraries
\texttt{numpy} and \texttt{scipy} were used for matrix and numerical manipulation, as well as \texttt{matplotlib} for
data visualization and graphical user interface.

Many options for this script are customizable via command line parameters. Appendix \ref{app:codehelp} includes a
description of all the possible parameters and a quick user guide.

\subsubsection{Feature selection}
The values for all the features from the annotated cells were analyzed with Weka \cite{hall2009weka}. The InfoGain
attribute evaluator was used to determine the information gain ratio for each feature, and the \todo{ad-hoc}top~4
features were chosen as the selected dimensions to represent the data for further analysis.

Due to the heterogeneity on the range of values between the selected features, spanning 
several orders of magnitude, normalization of the data was required. A standard z-score normalization was applied on
all the dimensions. This normalization, however, does not account for the fact that some data dimensions are more
important than others in terms of information gain. The relative information gain score among the group of
selected features was used to construct a scaling factor or weight for feature normalization, so that pairwise
distances over more important dimensions are smaller than distances over less important ones.
\begin{equation}
	w_d = \frac{\max{F}}{F_d}
\end{equation}

The selected features were: 

\begin{figure}[here]
\centering
\begin{tabular}{rlrr}
	ID & Description & Score & Weight\\
	\hline
	4 & Standard deviation of green intensities (whole cell) & 1.3275 & 1\\
	3 & Mean of green intensities (whole cell) & 1.1739 & 0.8843\\
	2 & Standard deviation of green intensities (nuclei) & 1.0605 & 0.7989\\
	1 & Mean of green intensities (nuclei) & 0.9868 & 0.7434\\
	92 & Information measure of correlation (scale 5, whole cell)? & 0.9288 & 0.6997\\
    53 & Correlation (scale 3, whole cell)? & 0.9241 & 0.6961\\
	93 & Maximal correlation coefficient (scale 5, whole cell) & 0.9081 & 0.6841
\end{tabular}
\end{figure}

\subsubsection{Data pre-processing}
Text files in both \texttt{arff} and \texttt{txt} formats containing feature information for labeled, soft-labeled,
and unlabeled data were read into a feature matrix $\mathbf M$ ($n$ data points $\times m$ features).

The \texttt{arff} files containing \emph{labeled data} were read and loaded into the feature matrix, by filtering the
read fields to include only the features obtained in the feature selection phase. The possible classes or labels are
loaded from the \texttt{arf{}f} file by parsing the line starting with \texttt{@attribute class} (this prefix can be
overriden via the \texttt{-{}-label-line-prefix} parameter in the command line). As a parameter to the program, a list
of ignored labels can be also passed with the option \texttt{-{}-ignored-labels}. Data points annotated with any of
these label identifiers will be left out of the feature matrix. No further sampling was performed over the labeled
data, i.e. all remaining (non-ignored) data points were kept.

The \texttt{txt} files containing \emph{soft-labeled data} were read a similar way, except there was no need for
parsing any formatting of the files. Each line in these \texttt{txt} files corresponds to a cell, and contains the
values for the features, space-separated, in the same ordering as the labeled files. To assign actual soft labels, the
relative file location in the file system was used as follows: the user indicates a root directory with all the 
soft-labeled data, and the files are expected in differents directories within, which are internally mapped 
(via a python dictionary) to the actual labels.

The \texttt{txt} files containing \emph{unlabeled data} were read exactly as described above for the soft-labeled
data. A default neutral label was assigned to all entries read from this files.

Due to the massive amount of information, sampling parameters over the soft-labeled and unlabeled data were
implemented. The command-line parameter \texttt{-{}-num-samples N} controls how many data points to use from both
soft-labeled and unlabeled data together ($N/2$ each). An additional flag parameter \texttt{-{}-class-sampling}
indicates that the script must sample the soft-labeled data uniformly over classes, to avoid sampling bias due to
large differences between the number of data points on each class.

As an outcome of this pre-processing step, the feature matrix containing the values of the selected features
for the labeled, soft-labeled, and unlabeled cells (after sampling, when specified) was returned, along with
the \todo{explain how to construct this}initial label matrix.

\subsubsection{Data normalization}
\lipsum[75]

\subsubsection{Graph construction}
The graph was internally represented by its weight matrix $\mathbf W$ ($\mathbf{W}_{ij}>0$ if there exists
an edge between the vertices $x_i$ and $x_j$, zero otherwise), plus a $n \times m$ label matrix $Y$ 
($n$ cells, $m$ possible labels or classes), with valid values ranging from $0$ to $1$. A value
of $Y_{i,j}=1$ represents complete confidence that the $i$-th cell in the data set, belongs to the $j$-th
phenotypic class of cells. Likewise, a value of $0$ indicates absolute disbelief that a cell corresponds to a
class, and values of $0.5$ indicate complete uncertainty about class membership.

\subsubsection{Label propagation}
\textcolor{gray}{\lipsum[44]}

\subsubsection{Hyperparameter optimization}
\lipsum[33]

\section{Results}
\textcolor{gray}{\lipsum[8]}

\section{Discussion}
\textcolor{gray}{\lipsum[9]}

\section{Conclusions}
\textcolor{gray}{\lipsum[3]}

%labeled and unlabeled datasources
%	\item[Tools] weka, numpy, scipy
%	\item[methods] label spreading
%\end{description}

\nocite{duda2001pattern}

\bibliographystyle{ieeetr}  % orders by occurrence in the document
\bibliography{references}

\appendix
\chapter{Features analyzed}\label{app:featurelist}
\begin{figure}[here]
%\centering

\begin{tabularx}{300pt}{| rX | rX|}
	\hline
	\multicolumn{2}{|c|}{\textbf{Position and intensity}} & \multicolumn{2}{c|}{\textbf{Nuclear shape features}}\\
	\hline
	1 & nuclei location $x$ & 11 & Area\\
	2 & nuclei location $y$ & 12 & Eccentricity\\

	3 & green mean intensity nuclei & 13 & Solidity\\
	4 & green std intensity nucleii & 14 & Extent\\

	5 & green mean intensity cells & 15 & Euler number\\
	6 & green std intensity cells & 16 & Perimeter\\

	7 & blue mean intensity nuclei & 17 & Form factor\\
	8 & blue std intensity nuclei & 18 & Major axis length\\

	9 & blue mean intensity cells & 19 & Minor axis length\\
	10 & blue std intensity cells & 20 & Orientation\\
	\hline
\end{tabularx}

\vspace*{0.4cm}
[Haralick features here]

%\begin{tabularx}{400pt}{|rrrrr|l}
%	\cline{1-5}
%	\multicolumn{3}{|c|}{Scale 3} & \multicolumn{2}{c|}{Scale 5}&\\
%	\cline{1-5}
%	\multicolumn{2}{|c|}{\textbf{Nuclei texture}} & \textbf{Cell texture} &
%	\textbf{Nuclei texture} & \textbf{Cell texture} &\\\cline{1-5}
%	green&blue&green&green&green&\\\cline{1-5}
%
%	21&36&51&66&81&Angular second moment\\
%	22&37&52&67&82&Contrast\\
%	23&38&53&68&83&Correlation\\
%	24&39&54&69&84&Variance\\
%	25&40&55&70&85&Inverse difference moment\\
%	26&41&56&71&86&Sum average\\
%	27&42&57&72&87&Sum variance\\ 
%	28&43&58&73&88&Sum entropy\\
%	29&44&59&74&89&Entropy\\
%	30&45&60&75&90&Difference variance\\
%	31&46&61&76&91&Difference entropy\\
%	32&47&62&77&92&Info meas 1-2\\
%	33&48&63&78&93&\\
%	34&49&64&79&94&Gabor X\&Y\\
%	35&50&65&80&95&\\
%	\cline{1-5}
%\end{tabularx}

%\begin{tabular}{rl}
%	1:2 & nuclei location\\
%
%	3 & green mean intensity nuclei\\
%	4 & green std intensity nucleii\\
%
%	5 & green mean intensity cells\\
%	6 & green std intensity cells\\
%
%	7 & blue mean intensity nuclei\\
%	8 & blue std intensity nuclei\\
%
%	9 & blue mean intensity cells\\
%	10 & blue std intensity cells\\
%
% shape descriptiors
%	   & \textbf Shape features from the nucleus\\
%	11 & Area\\
%	12 & Eccentricity\\
%	13 & Solidity\\
%	14 & Extent\\
%	15 & Euler number\\
%	16 & Perimeter\\
%	17 & Form factor\\
%	18 & Major axis length\\
%	19 & Minor axis length\\
%	20 & Orientation\\


% texture scale 3
	%21:35 & nuclei texture green\\
	%36:50 & nuclei texture blue\\
	%51:65 & cell texture green\\

% texture scale 5
	%66:80 & nuclei texture green\\
	%81:95 & cell texture green
%\end{tabular}
\end{figure}
%[Table with cell/nuclei intensity, shape and Haralick~\cite{haralick1973textural} texture features...]

\chapter{Script parameters and help}\label{app:codehelp}
\small
\todo{up to date?}
\begin{verbatim}
$ python hcs.py -h
usage: hcs.py [-h] [-t] [-l LABELED_FILE [LABELED_FILE ...]]
              [-u UNLABELED_FILE [UNLABELED_FILE ...]] [-s SOFT_LABELED_PATH]
              [-L NUM_LABELED_POINTS] [-n NUM_SAMPLES] [-c]
              [--max-iterations MAX_ITERATIONS] [-d WIDTH]
              [-nf {exp,knn3,knn4,knn5,knn6}]
              [-dm {euclidean,cityblock,cosine,sqeuclidean,hamming,chebyshev}]
              [-f FEATURE_INDEX [FEATURE_INDEX ...]] [-q]

Label propagation

optional arguments:
  -h, --help            show this help message and exit
  -t, --test            Performs a test run.
  -l LABELED_FILE [LABELED_FILE ...], --labeled LABELED_FILE [LABELED_FILE ...]
                        Labeled files.
  -u UNLABELED_FILE [UNLABELED_FILE ...], --unlabeled UNLABELED_FILE [UNLABELED_FILE ...]
                        Unlabeled files.
  -s SOFT_LABELED_PATH, --soft-labeled SOFT_LABELED_PATH
                        Path to soft labeled files. One directory per label
                        expected.
  -L NUM_LABELED_POINTS, --num-labeled NUM_LABELED_POINTS
                        Number of labeled data points to use. Default: use all
                        available
  -n NUM_SAMPLES, --num-samples NUM_SAMPLES
                        Number of samples. Default: 3000
  -c, --class-sampling  Distributes the number of samples given by
                        [NUM_SAMPLES] uniformly over all soft classes
  --max-iterations MAX_ITERATIONS
                        Maximum number of iterations. Default: 1000
  -d WIDTH, --display-columns WIDTH
                        Max width used for matrix display on console
  -nf {exp,knn3,knn4,knn5,knn6}, --neighborhood-function {exp,knn3,knn4,knn5,knn6}
                        Neighborhood function to use. Default: exp
  -dm {...}, --distance-metric {euclidean,cityblock,cosine,sqeuclidean,hamming,chebyshev}
                        Metric for calculating pairwise distances. Default:
                        euclidean
  -f FEATURE_INDEX [FEATURE_INDEX ...], --features FEATURE_INDEX [FEATURE_INDEX ...]
                        Selected feature indices (as given by the labeled
                        data).
  -q, --quiet           Displays progress and messages.
\end{verbatim}

\end{document}
